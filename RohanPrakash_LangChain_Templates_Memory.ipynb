{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohanprakash7/BostonHousePricing_End_to_End/blob/main/RohanPrakash_LangChain_Templates_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5208c400",
      "metadata": {
        "id": "5208c400"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 26px 28px 18px 28px; border-radius: 14px; margin-bottom: 22px; font-family: Arial, sans-serif;\">\n",
        "  <h2 style=\"margin-top: 0; font-size: 27px; letter-spacing: 0.5px;\">\n",
        "    ğŸ§  LangChain Lab 2: Prompt Templates &amp; Memory\n",
        "  </h2>\n",
        "  <p style=\"font-size: 17px; margin-bottom: 8px;\">\n",
        "    <span style=\"color: #a5d8ff;\">Instructor:</span> Prof. Dehghani\n",
        "  </p>\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 12px;\">Lab Overview</h3>\n",
        "  <p style=\"font-size: 16px; margin-bottom: 14px;\">\n",
        "    In this lab, you'll enhance your interactions with LLMs by using <b>Prompt Templates</b> and <b>Memory</b> features in LangChain.<br>\n",
        "    You'll learn to create <b>structured prompts dynamically</b> and maintain <b>conversation history</b> across multiple turns.\n",
        "  </p>\n",
        "  <hr style=\"border: 1px solid #3f77d4; margin: 16px 0;\">\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 10px;\">ğŸ¯ What You'll Learn</h3>\n",
        "  <ul style=\"font-size: 16px; margin: 0 0 10px 22px; line-height: 1.8;\">\n",
        "    <li>ğŸ”¹ <b>Prompt Templates</b> â€“ Format inputs dynamically for LLMs.</li>\n",
        "    <li>ğŸ”¹ <b>Memory in LangChain</b> â€“ Maintain context in multi-turn conversations.</li>\n",
        "    <li>ğŸ”¹ <b>Hands-on exercises</b> â€“ Reinforce concepts with practical coding tasks.</li>\n",
        "  </ul>\n",
        "  <p style=\"font-size: 15.5px; margin-top: 8px;\">\n",
        "    By the end, you'll be able to structure prompts effectively and implement conversational memory in LangChain applications. ğŸš€\n",
        "  </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##âš™ï¸ Install essential packages"
      ],
      "metadata": {
        "id": "8k2xSdP5989g"
      },
      "id": "8k2xSdP5989g"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e0f1adbd",
      "metadata": {
        "id": "e0f1adbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0894f24d-c1fd-461e-c80b-62cfbaf78a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/2.5 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/64.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m964.9/964.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#âš™ï¸ Install essential packages for LangChain with OpenAI & Gemini support\n",
        "\n",
        "!pip install -q --upgrade langchain                # Core LangChain framework for building LLM workflows\n",
        "!pip install -q --upgrade langchain-community      # Community integrations (still useful for many non-OpenAI/Gemini models)\n",
        "!pip install -q --upgrade langchain-openai         # âœ… NEW: Dedicated package for OpenAI integrations\n",
        "!pip install -q --upgrade langchain-google-genai   # Integration for Google's Gemini models\n",
        "!pip install -q --upgrade openai                   # OpenAI SDK for native API calls (not strictly needed for LangChain, but often useful)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ğŸ”‘ Step 2: Set Up OpenAI API Key"
      ],
      "metadata": {
        "id": "C9B4mXQr93u8"
      },
      "id": "C9B4mXQr93u8"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9f6983d2",
      "metadata": {
        "id": "9f6983d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74929968-63c0-45a8-b4b2-be36dadeaf72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… OpenAI API key loaded successfully!\n",
            "âœ… Google Gemini API key loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# âš™ï¸ Load API Keys from Colab Secrets\n",
        "# ==================================\n",
        "\n",
        "import os                                  # Used to set environment variables for API keys\n",
        "from google.colab import userdata          # To securely access stored secrets in Colab\n",
        "\n",
        "# Retrieve your stored secrets (API keys)\n",
        "OPENAI_API_KEY = userdata.get('OPEN_AI_API')   # OpenAI API key for GPT models\n",
        "GEMINI_API = userdata.get('GEMINI_API')   # Google Gemini API key for Gemini models\n",
        "\n",
        "# Set environment variables for the APIs and confirm success\n",
        "if OPENAI_API_KEY:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY   # Set OpenAI key as environment variable\n",
        "    print(\"âœ… OpenAI API key loaded successfully!\")\n",
        "else:\n",
        "    print(\"âŒ OpenAI API key not found. Please set 'OPEN_AI_API' in Colab secrets.\")\n",
        "\n",
        "if GEMINI_API:\n",
        "    os.environ[\"GEMINI_API\"] = GEMINI_API   # Set Gemini key as environment variable\n",
        "    print(\"âœ… Google Gemini API key loaded successfully!\")\n",
        "else:\n",
        "    print(\"âŒ Google Gemini API key not found. Please set 'GEMINI_API_KEY' in Colab secrets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5047273f",
      "metadata": {
        "id": "5047273f"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 26px 28px 18px 28px; border-radius: 14px; margin-bottom: 22px; font-family: Arial, sans-serif;\">\n",
        "  <h2 style=\"margin-top: 0; font-size: 25px; letter-spacing: 0.5px;\">ğŸ“ Prompt Templates in LangChain</h2>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 8px;\">ğŸ”¹ What are Prompt Templates?</h3>\n",
        "  <p style=\"font-size: 16px; margin-bottom: 12px;\">\n",
        "    Prompt Templates let you <b>dynamically format prompts</b> by inserting variables, making interactions with LLMs more flexible and reusable.<br>\n",
        "    Instead of writing static text, you can use placeholders that are filled in with real values at runtime.\n",
        "  </p>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 8px;\">ğŸ”¹ Why Use Prompt Templates?</h3>\n",
        "  <ul style=\"font-size: 16px; margin: 0 0 14px 22px; line-height: 1.8;\">\n",
        "    <li>âœ… <b>Reusability</b> â€“ No need for repetitive prompts.</li>\n",
        "    <li>âœ… <b>Dynamic Inputs</b> â€“ Easily personalize prompts with new user data.</li>\n",
        "    <li>âœ… <b>Consistency</b> â€“ Keeps your prompt formatting structured and reliable.</li>\n",
        "  </ul>\n",
        "\n",
        "  <h3 style=\"font-size: 17px; margin-bottom: 8px;\">ğŸ“Œ Example Usage</h3>\n",
        "  <div style=\"background: rgba(255,255,255,0.08); border-radius: 7px; padding: 11px 16px; margin-bottom: 6px;\">\n",
        "    <span style=\"color: #a5d8ff;\">Static prompt:</span><br>\n",
        "    <span style=\"font-family: 'Fira Mono', monospace; font-size: 15px; color: #fff;\">\n",
        "      \"What are the benefits of AI in healthcare?\"\n",
        "    </span>\n",
        "  </div>\n",
        "  <div style=\"background: rgba(255,255,255,0.08); border-radius: 7px; padding: 11px 16px;\">\n",
        "    <span style=\"color: #a5d8ff;\">Dynamic prompt with a template:</span><br>\n",
        "    <span style=\"font-family: 'Fira Mono', monospace; font-size: 15px; color: #fff;\">\n",
        "      \"What are the benefits of {technology} in {industry}?\"\n",
        "    </span><br>\n",
        "    <span style=\"font-size: 15px;\">If <b>{technology} = \"AI\"</b> and <b>{industry} = \"healthcare\"</b>, the prompt becomes:</span><br>\n",
        "    <span style=\"font-family: 'Fira Mono', monospace; font-size: 15px; color: #fff;\">\n",
        "      \"What are the benefits of AI in healthcare?\"\n",
        "    </span>\n",
        "  </div>\n",
        "\n",
        "  <p style=\"font-size: 16px; margin-top: 14px;\">ğŸš€ Let's get started with the first example!</p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7da3cc1a",
      "metadata": {
        "id": "7da3cc1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c98e859-9907-4149-8aef-4dfeb19eafbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ Generated Prompt: What are the benefits of Drones in SupplyChain in 1 sentence?\n",
            "ğŸ”¹ LLM Response: Drones speed up the delivery process in supply chains, reduce human labor, lower transportation costs, and are capable of reaching inaccessible areas.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# ==================================================\n",
        "# ğŸ¯ Using Prompt Templates with OpenAI (GPT-4)\n",
        "# ==================================================\n",
        "\"\"\"\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI   # âœ… NEW import for ChatOpenAI\n",
        "\n",
        "# Step 1: Define a prompt template with variables\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"technology\", \"industry\"],\n",
        "    template=\"What are the benefits of {technology} in {industry} in 1 sentence?\"\n",
        ")\n",
        "\n",
        "# Step 2: Format the prompt with specific values\n",
        "formatted_prompt = prompt_template.format(technology=\"Drones\", industry=\"SupplyChain\")\n",
        "\n",
        "# Step 3: Initialize the OpenAI LLM (GPT-4)\n",
        "llm_ChatGPT = ChatOpenAI(model_name=\"gpt-4\")\n",
        "\n",
        "# Step 4: Generate the response\n",
        "response_ChatGPT = llm_ChatGPT.invoke(formatted_prompt)\n",
        "\n",
        "# Step 5: Display results\n",
        "print(\"ğŸ”¹ Generated Prompt:\", formatted_prompt)\n",
        "print(\"ğŸ”¹ LLM Response:\", response_ChatGPT.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6cbb5ec1",
      "metadata": {
        "id": "6cbb5ec1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d83391cb-c073-449e-ed30-47a7ac8f6b9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ **Generated Prompt:** How does weight training impact gym in a few words?\n",
            "ğŸ”¹ **LLM Response:** Weight training enhances strength, improves body composition, promotes fat loss, boosts metabolism, increases athletic performance, and supports bone health in the gym.\n"
          ]
        }
      ],
      "source": [
        "# ==================================================\n",
        "# âœ‹ **Hands-On: Creating Dynamic Prompt Templates**\n",
        "# ==================================================\n",
        "\n",
        "# ğŸ“Œ **Task Instructions:**\n",
        "# 1ï¸âƒ£ Fill in the missing placeholders (-----) to complete the code.\n",
        "# 2ï¸âƒ£ Ensure the Prompt Template correctly replaces {topic} and {context}.\n",
        "# 3ï¸âƒ£ Run the code and verify GPT-4 generates a response.\n",
        "\n",
        "# âœ… Step 1: Define a Prompt Template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"topic\", \"context\"],  # Fill in the missing variable name\n",
        "    template=\"How does {topic} impact {context} in a few words?\"  # Structure of the prompt\n",
        ")\n",
        "\n",
        "# âœ… Step 2: Format the prompt with actual values\n",
        "formatted_prompt = prompt_template.format(topic=\"weight training\", context=\"gym\")\n",
        "\n",
        "# âœ… Step 3: Generate a response using OpenAI (GPT-4)\n",
        "llm_ChatGPT = ChatOpenAI(model_name=\"gpt-4\")  # Initialize the ChatGPT model\n",
        "response_ChatGPT = llm_ChatGPT.invoke(formatted_prompt)  # Fill in the correct variable for invoke\n",
        "\n",
        "# âœ… Step 4: Display results\n",
        "print(\"ğŸ”¹ **Generated Prompt:**\", formatted_prompt)\n",
        "print(\"ğŸ”¹ **LLM Response:**\", response_ChatGPT.content)  # Extract response content\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# ==================================================\n",
        "# ğŸ”„ Using Prompt Templates in a Loop\n",
        "# ==================================================\n",
        "\"\"\"\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI  # âœ… NEW importâ€”no deprecation warning!\n",
        "\n",
        "# Step 1ï¸âƒ£: Define a prompt template with variables\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"technology\", \"industry\"],\n",
        "    template=\"In one sentence, how does {technology} impact {industry} in 1 sentence?\"\n",
        ")\n",
        "\n",
        "# Step 2ï¸âƒ£: Initialize the OpenAI LLM (GPT-4)\n",
        "llm_ChatGPT = ChatOpenAI(model_name=\"gpt-4\")\n",
        "\n",
        "# Step 3ï¸âƒ£: Define input values for the loop\n",
        "input_data = [\n",
        "    {\"technology\": \"AI\", \"industry\": \"education\"},\n",
        "    {\"technology\": \"Blockchain\", \"industry\": \"finance\"},\n",
        "    {\"technology\": \"5G\", \"industry\": \"telecommunications\"},\n",
        "]\n",
        "\n",
        "# Step 4ï¸âƒ£: Loop through inputs, format the prompt, and generate a response\n",
        "for data in input_data:\n",
        "    formatted_prompt = prompt_template.format(**data)\n",
        "    response_ChatGPT = llm_ChatGPT.invoke(formatted_prompt)\n",
        "\n",
        "    # Step 5ï¸âƒ£: Display results in a clear, modern format\n",
        "    print(f\"ğŸ”¹ Prompt: {formatted_prompt}\")\n",
        "    print(f\"ğŸ’¡ Response: {response_ChatGPT.content}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "MQE2mEke-6_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc2a70c-4f26-4964-e19d-3e6456b3c68e"
      },
      "id": "MQE2mEke-6_w",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¹ Prompt: In one sentence, how does AI impact education in 1 sentence?\n",
            "ğŸ’¡ Response: AI impacts education by personalizing learning experiences, automating administrative tasks, and providing educators with tools to improve student understanding and engagement.\n",
            "------------------------------------------------------------\n",
            "ğŸ”¹ Prompt: In one sentence, how does Blockchain impact finance in 1 sentence?\n",
            "ğŸ’¡ Response: Blockchain technology transforms finance by providing a decentralized, secure and transparent system for transactions, reducing costs, increasing efficiency and eliminating the need for intermediaries.\n",
            "------------------------------------------------------------\n",
            "ğŸ”¹ Prompt: In one sentence, how does 5G impact telecommunications in 1 sentence?\n",
            "ğŸ’¡ Response: 5G revolutionizes telecommunications by offering significantly faster data speeds, lower latency, increased device capacity and improved reliability.\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background: linear-gradient(135deg, #1a386e 0%, #377ce8 100%); color: white; padding: 24px 26px 16px 26px; border-radius: 12px; margin-bottom: 22px; font-family: Arial, sans-serif;\">\n",
        "  <h3 style=\"margin-top: 0; font-size: 21px;\">ğŸ”— Wrapping Up: Why Prompt Templates Matter</h3>\n",
        "  <p style=\"font-size: 16px;\">\n",
        "    Just as you wouldn't rewrite a whole menu for every customer in a coffee shop, you don't need to create a new prompt for every question you ask an LLM.\n",
        "    <br><br>\n",
        "    <b>Prompt templates</b> give you a reusable, flexible, and structured way to interact with language modelsâ€”making your code cleaner, your queries more consistent, and your applications easier to scale.\n",
        "    <br><br>\n",
        "    Whether you're building a chatbot, automating business tasks, or analyzing data, prompt templates are an essential tool in your GenAI toolkit!\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "B9jXv_aeFukI"
      },
      "id": "B9jXv_aeFukI"
    },
    {
      "cell_type": "markdown",
      "id": "3c9bdf18",
      "metadata": {
        "id": "3c9bdf18"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 26px 28px 18px 28px; border-radius: 14px; margin-bottom: 22px; font-family: Arial, sans-serif;\">\n",
        "  <h2 style=\"margin-top: 0; font-size: 27px; letter-spacing: 0.5px;\">\n",
        "    ğŸ§  Understanding Memory in LangChain\n",
        "  </h2>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 10px;\">ğŸ”¹ What is Memory in LangChain?</h3>\n",
        "  <p style=\"font-size: 16px; margin-bottom: 13px;\">\n",
        "    By default, LLMs <b>do not remember past interactions</b>.<br>\n",
        "    LangChain <b>Memory</b> allows an AI model to <b>retain context</b> across multiple turns, enabling more natural, conversational interactions.\n",
        "  </p>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 10px;\">ğŸ”¹ Why Use Memory?</h3>\n",
        "  <ul style=\"font-size: 16px; margin: 0 0 14px 22px; line-height: 1.8;\">\n",
        "    <li>âœ… <b>Maintains conversation history</b> â€“ AI can recall previous exchanges.</li>\n",
        "    <li>âœ… <b>Improves response coherence</b> â€“ Reduces redundant user re-explanations.</li>\n",
        "    <li>âœ… <b>Essential for chatbots &amp; agents</b> â€“ Allows multi-turn dialogue without loss of context.</li>\n",
        "  </ul>\n",
        "\n",
        "  <h3 style=\"color: #a5d8ff; font-size: 19px; margin-bottom: 10px;\">ğŸ”¹ Types of Memory in LangChain</h3>\n",
        "  <ul style=\"font-size: 16px; margin: 0 0 14px 22px; line-height: 1.7;\">\n",
        "    <li>1ï¸âƒ£ <b>ConversationBufferMemory</b> â€“ Stores messages in a buffer (basic memory).</li>\n",
        "    <li>2ï¸âƒ£ <b>ConversationSummaryMemory</b> â€“ Summarizes past interactions instead of storing all messages.</li>\n",
        "    <li>3ï¸âƒ£ <b>ConversationBufferWindowMemory</b> â€“ Retains only the last N interactions for efficiency.</li>\n",
        "    <li>4ï¸âƒ£ <b>Vector-based Memory</b> â€“ Uses embeddings for advanced retrieval of past conversations.</li>\n",
        "  </ul>\n",
        "\n",
        "  <h3 style=\"font-size: 17px; margin-bottom: 8px;\">ğŸš€ What We'll Do in This Lab</h3>\n",
        "  <p style=\"font-size: 16px;\">\n",
        "    Weâ€™ll start with <b>ConversationBufferMemory</b>, which allows an LLM to <b>recall past messages</b> and interact in a more natural, memory-enhanced way.<br>\n",
        "    <br>\n",
        "    <span style=\"color: #a5d8ff;\">Note:</span> When using a conversation chain with memory, youâ€™ll use the <b><code>predict()</code></b> method instead of <code>invoke()</code>. This lets the AI maintain and use context across multiple turns.<br>\n",
        "    <br>\n",
        "    Let's get started! ğŸ‘‡\n",
        "  </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "225bb6e6",
      "metadata": {
        "id": "225bb6e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e6fa5ea-57a8-4e53-8cca-82c779d1bbe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== WITHOUT MEMORY (Stateless LLM) ==========\n",
            "ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: Our anniversary is October 5th. Please remember that!\n",
            "ğŸ¤– AnniversaryBot: I will certainly remember that! Happy early anniversary! If you need any help planning or organizing anything for the special day, feel free to ask.\n",
            "ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: When is our anniversary?\n",
            "ğŸ¤– AnniversaryBot: As an AI, I don't have personal relationships or anniversaries. But I was launched by OpenAI in June 2020.\n",
            "\n",
            "========== WITH MEMORY ==========\n",
            "ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: Our anniversary is October 5th. Please remember that!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2823701360.py:31: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "/tmp/ipython-input-2823701360.py:33: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation_with_mem = ConversationChain(llm=llm_with_mem, memory=memory)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– AnniversaryBot: Of course, I will keep a note of it! October 5th is your anniversary. Should I remind you a few days before the date or on the day itself?\n",
            "ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: When is our anniversary?\n",
            "ğŸ¤– AnniversaryBot: Your anniversary is on October 5th.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# ==================================================\n",
        "# ğŸ’¬ğŸ§  Memory Matters: AnniversaryBot Demo (Stateless vs. Memory)\n",
        "# ==================================================\n",
        "#\n",
        "# This demo shows how LangChain's memory feature allows an AI assistant to remember\n",
        "# detailsâ€”using the example of a user telling the bot their anniversary date.\n",
        "\"\"\"\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# -------------------------------\n",
        "# 1ï¸âƒ£ Version WITHOUT Memory (Stateless)\n",
        "# -------------------------------\n",
        "llm_stateless = ChatOpenAI(model_name=\"gpt-4\")\n",
        "\n",
        "print(\"\\n========== WITHOUT MEMORY (Stateless LLM) ==========\")\n",
        "print(\"ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: Our anniversary is October 5th. Please remember that!\")\n",
        "response1 = llm_stateless.invoke(\"Our anniversary is October 5th. Please remember that!\")\n",
        "print(\"ğŸ¤– AnniversaryBot:\", response1.content)\n",
        "\n",
        "print(\"ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: When is our anniversary?\")\n",
        "response2 = llm_stateless.invoke(\"When is our anniversary?\")\n",
        "print(\"ğŸ¤– AnniversaryBot:\", response2.content)\n",
        "\n",
        "# -------------------------------\n",
        "# 2ï¸âƒ£ Version WITH Memory\n",
        "# -------------------------------\n",
        "memory = ConversationBufferMemory()\n",
        "llm_with_mem = ChatOpenAI(model_name=\"gpt-4\")\n",
        "conversation_with_mem = ConversationChain(llm=llm_with_mem, memory=memory)\n",
        "\n",
        "print(\"\\n========== WITH MEMORY ==========\")\n",
        "print(\"ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: Our anniversary is October 5th. Please remember that!\")\n",
        "response3 = conversation_with_mem.predict(input=\"Our anniversary is October 5th. Please remember that!\")\n",
        "print(\"ğŸ¤– AnniversaryBot:\", response3)\n",
        "\n",
        "print(\"ğŸ‘©â€â¤ï¸â€ğŸ‘¨ User: When is our anniversary?\")\n",
        "response4 = conversation_with_mem.predict(input=\"When is our anniversary?\")\n",
        "print(\"ğŸ¤– AnniversaryBot:\", response4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9dc20167",
      "metadata": {
        "id": "9dc20167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d974bf23-3b17-4475-e8f6-0d7675375079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¬ **Retailer:** Last week, the customer demand was 200 units. What should I order this week?\n",
            "ğŸ¤– **ChatGPT:** As an AI, I don't have real-time data to predict the exact customer demand for this week. However, generally, businesses use sales forecasting methods relying on historical data and current market trends. If last week's demand was 200 units and you have no reason to anticipate a significant change, a similar amount could be a safe starting point. It's always a good idea to consult with your sales team or use predictive analytics tools for more accurate forecasting.\n",
            "\n",
            "ğŸ’¬ **Retailer:** If demand increases by 10%, how many units should I prepare for next week?\n",
            "ğŸ¤– **ChatGPT:** If demand increases by 10% and last week's demand was 200 units, you should prepare for an increase of 20 units (10% of 200). So, you would need a total of 220 units for next week.\n",
            "\n",
            "ğŸ’¬ **Retailer:** What was the demand I mentioned last week?\n",
            "ğŸ¤– **ChatGPT:** Last week, you mentioned that the customer demand was 200 units.\n"
          ]
        }
      ],
      "source": [
        "# ==================================================\n",
        "# âœ‹ Hands-On: Using Memory with OpenAI (Beer Game - Supply Chain Predictions)\n",
        "# ==================================================\n",
        "#\n",
        "# ğŸ“Œ Task Instructions:\n",
        "# 1ï¸âƒ£ Fill in the missing placeholders (-----) to complete the code.\n",
        "# 2ï¸âƒ£ Ensure the AI remembers previous demand data and predicts future order quantities.\n",
        "# 3ï¸âƒ£ Run the code and check if ChatGPT maintains context for supply chain decisions.\n",
        "\n",
        "# âœ… Step 1: Initialize Memory\n",
        "memory = ConversationBufferMemory()  # Initialize the correct memory class\n",
        "\n",
        "# âœ… Step 2: Initialize ChatGPT with Memory\n",
        "llm_ChatGPT = ChatOpenAI(model_name=\"gpt-4\")  # Initialize ChatGPT model\n",
        "conversation = ConversationChain(llm=llm_ChatGPT, memory=memory)  # Attach memory to conversation\n",
        "\n",
        "# âœ… Step 3: Run Multiple Interactions\n",
        "print(\"\\nğŸ’¬ **Retailer:** Last week, the customer demand was 200 units. What should I order this week?\")\n",
        "response_ChatGPT = conversation_with_mem.predict(input=\"Last week, the customer demand was 200 units. What should I order this week?\")  # Call the correct method\n",
        "print(\"ğŸ¤– **ChatGPT:**\", response_ChatGPT)\n",
        "\n",
        "print(\"\\nğŸ’¬ **Retailer:** If demand increases by 10%, how many units should I prepare for next week?\")\n",
        "response_ChatGPT = conversation_with_mem.predict(input=\"If demand increases by 10%, how many units should I prepare for next week?\")\n",
        "print(\"ğŸ¤– **ChatGPT:**\", response_ChatGPT)\n",
        "\n",
        "print(\"\\nğŸ’¬ **Retailer:** What was the demand I mentioned last week?\")\n",
        "response_ChatGPT = conversation_with_mem.predict(input=\"What was the demand I mentioned last week?\")\n",
        "print(\"ğŸ¤– **ChatGPT:**\", response_ChatGPT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using 'Summarized Conversation' Example\n"
      ],
      "metadata": {
        "id": "3ihDbUh4E5Sn"
      },
      "id": "3ihDbUh4E5Sn"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# ğŸ¤ **Using Memory in LangChain: Job Interview Prep**\n",
        "# ==================================================\n",
        "#\n",
        "# This script simulates a job interview practice session.\n",
        "# It uses ConversationSummaryMemory to retain key points from previous exchanges\n",
        "# rather than storing the full conversation history.\n",
        "\n",
        "# âœ… Import required classes\n",
        "from langchain.memory import ConversationSummaryMemory  # Summarized conversation memory\n",
        "\n",
        "# âœ… Step 1: Initialize Memory\n",
        "# This memory will maintain a **summarized** version of the conversation.\n",
        "memory = ConversationSummaryMemory(llm=ChatOpenAI(model_name=\"gpt-4\"))\n",
        "\n",
        "# âœ… Step 2: Initialize ChatGPT with Memory\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\")  # Using GPT-4 model\n",
        "\n",
        "# âœ… Step 3: Initialize Conversation Chain\n",
        "# The model will summarize key details from the job interview practice.\n",
        "conversation = ConversationChain(llm=llm, memory=memory)\n",
        "\n",
        "# âœ… Step 4: Conduct the Interview Simulation\n",
        "\n",
        "print(\"\\nğŸ’¬ **User:** Can you ask me a common interview question?\")\n",
        "response = conversation.predict(input=\"Can you ask me a common interview question?\")\n",
        "print(\"ğŸ¤– **ChatGPT:**\", response)\n",
        "\n",
        "# âœ… Check memory after first interaction\n",
        "print(\"\\nğŸ“œ **Memory Summary After 1st Question:**\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n",
        "\n",
        "print(\"\\nğŸ’¬ **User:** My biggest strength is adaptability and problem-solving.\")\n",
        "response = conversation.predict(input=\"My biggest strength is adaptability and problem-solving.\")\n",
        "print(\"ğŸ¤– **ChatGPT:**\", response)\n",
        "\n",
        "# âœ… Check memory after user shares strength\n",
        "print(\"\\nğŸ“œ **Memory Summary After Strength Response:**\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n",
        "\n",
        "print(\"\\nğŸ’¬ **User:** My biggest weakness is that I sometimes overthink decisions.\")\n",
        "response = conversation.predict(input=\"My biggest weakness is that I sometimes overthink decisions.\")\n",
        "print(\"ğŸ¤– **ChatGPT:**\", response)\n",
        "\n",
        "# âœ… Check memory after user shares weakness\n",
        "print(\"\\nğŸ“œ **Memory Summary After Weakness Response:**\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n",
        "\n",
        "print(\"\\nğŸ’¬ **User:** Can you summarize what we discussed so far?\")\n",
        "response = conversation.predict(input=\"Can you summarize what we discussed so far?\")\n",
        "print(\"ğŸ¤– **ChatGPT:**\", response)\n",
        "\n",
        "# âœ… Final Memory Check\n",
        "print(\"\\nğŸ“œ **Final Memory Summary:**\")\n",
        "print(memory.load_memory_variables({})[\"history\"])\n"
      ],
      "metadata": {
        "id": "PGhKAmB-E3vm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d4d2072-c6c0-4ffd-820f-87051557ac9c"
      },
      "id": "PGhKAmB-E3vm",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-809088591.py:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(llm=ChatOpenAI(model_name=\"gpt-4\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ’¬ **User:** Can you ask me a common interview question?\n",
            "ğŸ¤– **ChatGPT:** Sure, one of the common interview questions is: Can you describe a challenging situation you encountered at work and how you handled it?\n",
            "\n",
            "ğŸ“œ **Memory Summary After 1st Question:**\n",
            "The human asks the AI to pose a common interview question. The AI asks the human to describe a challenging situation they encountered at work and how they handled it.\n",
            "\n",
            "ğŸ’¬ **User:** My biggest strength is adaptability and problem-solving.\n",
            "ğŸ¤– **ChatGPT:** That's a great strength to have, especially in today's ever-changing work environment. Could you provide a specific example where you had to adapt quickly and solve a problem at your workplace?\n",
            "\n",
            "ğŸ“œ **Memory Summary After Strength Response:**\n",
            "The human asks the AI to pose a common interview question. The AI inquires about the human's response to a challenging workplace situation. The human responds by stating their biggest strength is adaptability and problem-solving, to which the AI asks for a specific example of this strength in action in an ever-changing work environment.\n",
            "\n",
            "ğŸ’¬ **User:** My biggest weakness is that I sometimes overthink decisions.\n",
            "ğŸ¤– **ChatGPT:** That is an interesting point, sometimes introspection can turn into overthinking which might lead to delay in decision making. However, overthinking can also mean that you consider all possible outcomes before making a final decision. Could you share an instance where this trait affected a work situation - either positively or negatively?\n",
            "\n",
            "ğŸ“œ **Memory Summary After Weakness Response:**\n",
            "The human asks the AI to pose a common interview question. The AI inquires about the human's response to a challenging workplace situation. The human responds by stating their biggest strength is adaptability and problem-solving, to which the AI asks for a specific example. The human admits their biggest weakness is overthinking decisions. The AI points out that overthinking can cause decision-making delays, but also allows for consideration of all outcomes. The AI further asks for an example of where this trait has affected a work situation positively or negatively.\n",
            "\n",
            "ğŸ’¬ **User:** Can you summarize what we discussed so far?\n",
            "ğŸ¤– **ChatGPT:** Sure! We had an interview-like conversation. You asked me to provide a typical interview question and I asked how you handle challenging workplace situations. You replied that you believe your biggest strength is adaptability and your ability to problem-solve, of which I requested a concrete example but you chose to share your biggest weakness instead â€” that you tend to overthink decisions. I noted that your tendency to overthink can lead to delays, but on the other side, it can also help you to consider all the possible outcomes before making a decision. To continue our discussion, I then asked you to give an example of a situation at work where overthinking impacted the outcome either positively or negatively.\n",
            "\n",
            "ğŸ“œ **Final Memory Summary:**\n",
            "The human asks the AI to pose a common interview question. The AI inquires about the human's response to a challenging workplace situation. The human responds by stating their biggest strength is adaptability and problem-solving, to which the AI asks for a specific example. The human admits their biggest weakness is overthinking decisions. The AI points out that overthinking can cause decision-making delays, but also allows for consideration of all outcomes. The AI further asks for an example of where this trait has affected a work situation positively or negatively. On being asked to summarise their discussion, the AI recaps the conversation from the interview-like question, the human's strengths, weakness, and the impacts of overthinking on decision-making.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# ğŸº **LangChain Beer Game: Comparing Memory Types\n",
        "# ==================================================\n",
        "#\n",
        "# This script simulates a Beer Game ordering process over 6 weeks.\n",
        "# It uses:\n",
        "# 1ï¸âƒ£ ConversationBufferMemory (Tracks full history)\n",
        "# 2ï¸âƒ£ ConversationBufferWindowMemory (Tracks only last 3 orders)\n",
        "#\n",
        "# The AI predicts the next order quantity based on past interactions.\n",
        "\n",
        "# âœ… Import required libraries\n",
        "import pandas as pd\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# âœ… Step 1: Initialize Memory Types\n",
        "buffer_memory = ConversationBufferMemory(return_messages=True)  # Stores entire history\n",
        "window_memory = ConversationBufferWindowMemory(k=3, return_messages=True)  # Stores last 3 interactions\n",
        "\n",
        "# âœ… Step 2: Initialize Chat Model (NEW!)\n",
        "llm = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "# âœ… Step 3: Define a Prompt Template\n",
        "beer_game_template = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=\"\"\"\n",
        "    You are managing a supply chain for a beer distribution system.\n",
        "    Orders fluctuate at first but stabilize later.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Based on past orders, what should be the next order quantity?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "# âœ… Step 4: Define Processing Pipelines\n",
        "buffer_chain = beer_game_template | llm\n",
        "window_chain = beer_game_template | llm\n",
        "\n",
        "# âœ… Step 5: Define Order Fluctuations (First 3 weeks volatile, last 3 weeks stable)\n",
        "weekly_orders = [20, 50, 10, 25, 30, 30]  # Example fluctuations\n",
        "\n",
        "# Store results for comparison\n",
        "buffer_memory_log = []\n",
        "window_memory_log = []\n",
        "buffer_predictions = []\n",
        "window_predictions = []\n",
        "\n",
        "# âœ… Step 6: Run the Simulation\n",
        "for week in range(1, len(weekly_orders) + 1):\n",
        "    prev_orders = \", \".join(map(str, weekly_orders[:week]))  # Orders seen so far\n",
        "    context = f\"Week {week}: The previous orders were {prev_orders}.\"\n",
        "\n",
        "    # Store input in memory\n",
        "    buffer_memory.save_context({\"context\": context}, {\"response\": \"\"})\n",
        "    window_memory.save_context({\"context\": context}, {\"response\": \"\"})\n",
        "\n",
        "    # Get AI predictions using RunnableSequence\n",
        "    buffer_prediction = buffer_chain.invoke({\"context\": context})\n",
        "    window_prediction = window_chain.invoke({\"context\": context})\n",
        "\n",
        "    # Retrieve memory states\n",
        "    buffer_memory_summary = buffer_memory.load_memory_variables({})[\"history\"]\n",
        "    window_memory_summary = window_memory.load_memory_variables({})[\"history\"]\n",
        "\n",
        "    # Store memory states and predictions\n",
        "    buffer_memory_log.append(buffer_memory_summary)\n",
        "    window_memory_log.append(window_memory_summary)\n",
        "    buffer_predictions.append(buffer_prediction.content)\n",
        "    window_predictions.append(window_prediction.content)\n",
        "\n",
        "# âœ… Step 7: Display Results in a Table\n",
        "df = pd.DataFrame({\n",
        "    \"Week\": list(range(1, len(weekly_orders) + 1)),\n",
        "    \"Actual Orders\": weekly_orders,\n",
        "    \"Buffer Memory (Stores All)\": buffer_memory_log,\n",
        "    \"Window Memory (Last 3 Turns)\": window_memory_log,\n",
        "    \"Buffer Memory Prediction\": buffer_predictions,\n",
        "    \"Window Memory Prediction\": window_predictions\n",
        "})\n"
      ],
      "metadata": {
        "id": "uFTXdRyVFUNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb91511a-6869-4a62-9239-47e76566d8be"
      },
      "id": "uFTXdRyVFUNC",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1262663304.py:20: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  window_memory = ConversationBufferWindowMemory(k=3, return_messages=True)  # Stores last 3 interactions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df)"
      ],
      "metadata": {
        "id": "dRm1ZzStWDLa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "47e87f67-5703-47ae-f26c-a912a012d347"
      },
      "id": "dRm1ZzStWDLa",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Week  Actual Orders                         Buffer Memory (Stores All)  \\\n",
              "0     1             20  [content='Week 1: The previous orders were 20....   \n",
              "1     2             50  [content='Week 1: The previous orders were 20....   \n",
              "2     3             10  [content='Week 1: The previous orders were 20....   \n",
              "3     4             25  [content='Week 1: The previous orders were 20....   \n",
              "4     5             30  [content='Week 1: The previous orders were 20....   \n",
              "5     6             30  [content='Week 1: The previous orders were 20....   \n",
              "\n",
              "                        Window Memory (Last 3 Turns)  \\\n",
              "0  [content='Week 1: The previous orders were 20....   \n",
              "1  [content='Week 1: The previous orders were 20....   \n",
              "2  [content='Week 1: The previous orders were 20....   \n",
              "3  [content='Week 2: The previous orders were 20,...   \n",
              "4  [content='Week 3: The previous orders were 20,...   \n",
              "5  [content='Week 4: The previous orders were 20,...   \n",
              "\n",
              "                            Buffer Memory Prediction  \\\n",
              "0  Without more information provided such as week...   \n",
              "1  The order quantity seems to be increasing. Whi...   \n",
              "2  The next order quantity would ideally be based...   \n",
              "3  The next order quantity is difficult to predic...   \n",
              "4  Although I am an AI trained for text predictio...   \n",
              "5  One common strategy used in forecasting future...   \n",
              "\n",
              "                            Window Memory Prediction  \n",
              "0  As a language model AI developed by OpenAI, I ...  \n",
              "1  The order quantity trend is increasing. Howeve...  \n",
              "2  The average order quantity over the past three...  \n",
              "3  The order quantities seem relatively unpredict...  \n",
              "4  This depends entirely on the method used for f...  \n",
              "5  Making predictions based on past orders can of...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e0be85a-12cc-4800-94aa-4ae5815b49b0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Week</th>\n",
              "      <th>Actual Orders</th>\n",
              "      <th>Buffer Memory (Stores All)</th>\n",
              "      <th>Window Memory (Last 3 Turns)</th>\n",
              "      <th>Buffer Memory Prediction</th>\n",
              "      <th>Window Memory Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>Without more information provided such as week...</td>\n",
              "      <td>As a language model AI developed by OpenAI, I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>50</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>The order quantity seems to be increasing. Whi...</td>\n",
              "      <td>The order quantity trend is increasing. Howeve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>The next order quantity would ideally be based...</td>\n",
              "      <td>The average order quantity over the past three...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>25</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>[content='Week 2: The previous orders were 20,...</td>\n",
              "      <td>The next order quantity is difficult to predic...</td>\n",
              "      <td>The order quantities seem relatively unpredict...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>30</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>[content='Week 3: The previous orders were 20,...</td>\n",
              "      <td>Although I am an AI trained for text predictio...</td>\n",
              "      <td>This depends entirely on the method used for f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>30</td>\n",
              "      <td>[content='Week 1: The previous orders were 20....</td>\n",
              "      <td>[content='Week 4: The previous orders were 20,...</td>\n",
              "      <td>One common strategy used in forecasting future...</td>\n",
              "      <td>Making predictions based on past orders can of...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e0be85a-12cc-4800-94aa-4ae5815b49b0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8e0be85a-12cc-4800-94aa-4ae5815b49b0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8e0be85a-12cc-4800-94aa-4ae5815b49b0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7d6eb286-21f2-47ff-aa7e-bda617ad04a1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7d6eb286-21f2-47ff-aa7e-bda617ad04a1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7d6eb286-21f2-47ff-aa7e-bda617ad04a1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_07bede7c-3ebb-4b3b-b375-39e02f7ed456\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_07bede7c-3ebb-4b3b-b375-39e02f7ed456 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"Week\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 6,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          1,\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual Orders\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 10,\n        \"max\": 50,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          50,\n          30,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buffer Memory (Stores All)\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Window Memory (Last 3 Turns)\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buffer Memory Prediction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Without more information provided such as weekly demand trends, growth rates, or even the orders for subsequent weeks, it's challenging to accurately predict the next order quantity. However, if we assume that the orders fluctuate but eventually stabilize around the same number, a safe bet would be to order an amount similar to previous orders, which is 20. Please provide more details for a more accurate estimate.\",\n          \"The order quantity seems to be increasing. While it's hard to predict with only two data points, one possible strategy could be to continue the pattern and increase the order quantity by the same amount as the previous increase. The increase from 20 to 50 is 30, so adding 30 to the second order gives us an estimate of 80 for the next order quantity. \\n\\nHowever, it is advisable to consider other factors like storage capacity, demand prediction, and sales trends before making the decision.\",\n          \"One common strategy used in forecasting future demand based on past patterns is the Moving Average method, which calculates the average of a certain number of previous periods, usually the most recent.\\n\\nFor simplicity, let's take the last 3 weeks to calculate moving average:\\n\\nWeek 4 order: 25\\n\\nWeek 5 order: 30\\n\\nWeek 6 order: 30\\n\\nMoving average: (25 + 30 + 30) / 3 = 28.33\\n\\nIt seems like the order quantity is stabilizing around 30, so the next order quantity could be around 30 or to be more precise 28 or 29 if we should consider decimal points. However, a more holistic approach might consider other factors like seasonal influences or promotional activities that might affect demand.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Window Memory Prediction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"As a language model AI developed by OpenAI, I don't have any new data to analyze or predict the next order quantity accurately. However, a common approach in supply chain management typically involves using data on past orders to predict future orders. If the order quantity has been stabilizing around 20, then it would be reasonable to continue with the same quantity until you obtain further data indicating a change in demand. If the orders are expected to increase due to any reason (like a festival, event, etc.), then it should be factored into the decision. Alternatively, you can also consider factors such as the current inventory level, lead-time, and safety stock into account when deciding the order quantity. Consideration of these factors usually leads to better decision making in supply chain management.\",\n          \"The order quantity trend is increasing. However, with only two data points, it's hard to accurately predict the next order quantity. Considering the existing pattern, one might expect the next order to be higher than the last. However, since the orders are said to stabilize later, the third week might not continue the increasing trend significantly. The next order could potentially be around 60 or remain similar to that of week two (50), but it ultimately depends on more specific forecasts about customer demand and market trends.\",\n          \"Making predictions based on past orders can oftentimes be inaccurate due to many unforeseen factors. However, assuming the orders have indeed stabilized, it might be safe to order around 30 units - this being the recent consistent order quantity. Please note this doesn't account for growth, decrease, or other business factors.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Save the table to an Excel file\n",
        "df.to_excel(\"beer_game_memory_comparison.xlsx\", index=False)\n",
        "\n",
        "# âœ… Print confirmation message\n",
        "print(\"Data saved to 'beer_game_memory_comparison.xlsx'\")\n"
      ],
      "metadata": {
        "id": "tlLyE0DYB6Qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75939eeb-799d-44f2-d059-af6f6bbb56eb"
      },
      "id": "tlLyE0DYB6Qt",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to 'beer_game_memory_comparison.xlsx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“Œ **Assignment: AI Stock Market Trend Prediction with Memory**\n",
        "\n",
        "## **Objective**\n",
        "In this assignment, you will use AI to predict stock market trends based on historical stock prices. You will compare how different memory types affect AI's ability to track and predict future trends.\n",
        "\n",
        "## **Tasks**\n",
        "1. **Initialize memory types** (`ConversationBufferMemory` and `ConversationBufferWindowMemory`).\n",
        "2. **Define the AI model** (GPT-4 or another suitable model).\n",
        "3. **Complete the prompt template** to guide AI predictions.\n",
        "4. **Process stock price data** and use memory to store past trends.\n",
        "5. **Retrieve and analyze stored memory** after each step.\n",
        "6. **Invoke the AI model correctly** to generate predictions.\n",
        "7. **Save results to an Excel file** for analysis.\n",
        "\n",
        "## **Expected Outcome**\n",
        "You will observe how AI predictions change when it has full history vs. limited memory. This will help you understand the impact of memory in AI-based forecasting.\n",
        "\n",
        "ğŸš€ **Complete the placeholders and run the script to generate insights!** ğŸš€\n"
      ],
      "metadata": {
        "id": "r9Y518a4MeVJ"
      },
      "id": "r9Y518a4MeVJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# âœ‹ **Hands-On: Creating Dynamic Prompt Templates**\n",
        "# ğŸ“ˆ **AI Assignment: Stock Market Trend Prediction with Memory**\n",
        "# ==================================================\n",
        "#\n",
        "# ğŸ”¹ In this assignment, you will use AI to predict stock market trends.\n",
        "# ğŸ”¹ You will compare how different memory types affect AI's ability to track stock price movements.\n",
        "# ğŸ”¹ Complete the placeholders (----) to make the script functional.\n",
        "#\n",
        "# ğŸ“Œ **Your Tasks:**\n",
        "# 1ï¸âƒ£ Initialize the correct memory types.\n",
        "# 2ï¸âƒ£ Define the AI model.\n",
        "# 3ï¸âƒ£ Complete the template prompt.\n",
        "# 4ï¸âƒ£ Use memory correctly when processing stock data.\n",
        "# 5ï¸âƒ£ Ensure correct invocation of AI for predictions.\n",
        "# 6ï¸âƒ£ Retrieve and analyze stored memory.\n",
        "# 7ï¸âƒ£ Save results in an Excel file.\n",
        "\n",
        "# âœ… Import required libraries\n",
        "import pandas as pd\n",
        "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory  # Import appropriate memory classes\n",
        "from langchain_openai import ChatOpenAI  # Import ChatGPT model\n",
        "from langchain.prompts import PromptTemplate  # Import PromptTemplate\n",
        "# âœ… Step 1: Initialize Memory Types\n",
        "buffer_memory = buffer_memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)  # Stores full stock history\n",
        "window_memory = window_memory = ConversationBufferWindowMemory(k=3, memory_key=\"history\", return_messages=True)  # Stores last 3 stock movements\n",
        "\n",
        "# âœ… Step 2: Initialize Chat Model\n",
        "llm = ChatOpenAI(model=\"gpt-4\") # Define the AI model (GPT-4 or another model)\n",
        "\n",
        "# âœ… Step 3: Define a Prompt Template\n",
        "stock_prediction_template = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=\"\"\"\n",
        "    You are an AI financial analyst predicting stock market trends.\n",
        "\n",
        "    {context}\n",
        "\n",
        "    Based on this stock price history, what will be the next trend (Up, Down, or Stable)?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# âœ… Step 4: Define Processing Pipelines\n",
        "buffer_chain = buffer_chain = stock_prediction_template | llm # Define how memory connects to AI\n",
        "window_chain = buffer_chain = stock_prediction_template | llm # Define how memory connects to AI with windowed memory\n",
        "\n",
        "# âœ… Step 5: Define Stock Price Data (Fluctuations in the first weeks, then stabilizing)\n",
        "stock_prices = [120, 125, 110, 130, 128, 129]  # Example price movements\n",
        "\n",
        "# Store results for comparison\n",
        "buffer_memory_log = []\n",
        "window_memory_log = []\n",
        "buffer_predictions = []\n",
        "window_predictions = []\n",
        "\n",
        "# âœ… Step 6: Run the Prediction Simulation\n",
        "for week in range(1, len(stock_prices) + 1):\n",
        "    prev_prices = \", \".join(map(str, stock_prices[:week]))  # Stocks seen so far\n",
        "    context = f\"Week {week}: The previous stock prices were {prev_prices}.\"\n",
        "\n",
        "    # Store input in memory\n",
        "    buffer_memory.chat_memory.add_user_message(context)  # Store context in buffer memory\n",
        "    window_memory.chat_memory.add_user_message(context)  # Store context in windowed memory\n",
        "\n",
        "    # Get AI predictions\n",
        "    buffer_prediction = buffer_chain.invoke({\"context\": context}) # Invoke AI for buffer memory\n",
        "    window_prediction = window_chain.invoke({\"context\": context})  # Invoke AI for window memory\n",
        "\n",
        "    # Retrieve memory states\n",
        "    buffer_memory_summary = buffer_memory.load_memory_variables({})[\"history\"]\n",
        "    window_memory_summary = window_memory.load_memory_variables({})[\"history\"]\n",
        "\n",
        "    # Store memory states and predictions\n",
        "    buffer_memory_log.append(buffer_memory_summary)\n",
        "    window_memory_log.append(window_memory_summary)\n",
        "    buffer_predictions.append(buffer_prediction.content)\n",
        "    window_predictions.append(window_prediction.content)\n",
        "\n",
        "# âœ… Step 7: Save Results in an Excel File\n",
        "df = pd.DataFrame({\n",
        "    \"Week\": list(range(1, len(stock_prices) + 1)),\n",
        "    \"Stock Price\": stock_prices,\n",
        "    \"Buffer Memory (Stores All)\": buffer_memory_log,\n",
        "    \"Window Memory (Last 3 Turns)\": window_memory_log,\n",
        "    \"Buffer Memory Prediction\": buffer_predictions,\n",
        "    \"Window Memory Prediction\": window_predictions\n",
        "})\n",
        "\n",
        "df.to_excel(\"stock_market_memory_comparison.xlsx\", index=False)\n",
        "\n",
        "\n",
        "print(\"Assignment completed! Data saved to 'stock_market_memory_comparison.xlsx'\")\n"
      ],
      "metadata": {
        "id": "TGapDKV0HJWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad57229-2a08-4e07-d4f8-2b936926b588"
      },
      "id": "TGapDKV0HJWg",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assignment completed! Data saved to 'stock_market_memory_comparison.xlsx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mEl7PWWgYhVC"
      },
      "id": "mEl7PWWgYhVC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}